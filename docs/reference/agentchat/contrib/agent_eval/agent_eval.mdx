---
sidebarTitle: agent_eval
title: autogen.agentchat.contrib.agent_eval.agent_eval
---

## Functions

<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>
#### generate_criteria
<a href="#autogen.agentchat.contrib.agent_eval.agent_eval..generate_criteria" class="headerlink" title="Permanent link"></a>

```python
generate_criteria(
    llm_config: dict | Literal[False] | None = None,
    task: autogen.agentchat.contrib.agent_eval.task.Task = None,
    additional_instructions: str = '',
    max_round=2,
    use_subcritic: bool = False
) -> 
```

    Creates a list of criteria for evaluating the utility of a given task.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `llm_config` | llm inference configuration.<br/><br/>**Type:** `dict \| Literal[False] \| None`<br/><br/>**Default:** None |
| `task` | The task to evaluate.<br/><br/>**Type:** `autogen.agentchat.contrib.agent_eval.task.Task`<br/><br/>**Default:** None |
| `additional_instructions` | Additional instructions for the criteria agent.<br/><br/>**Type:** `str`<br/><br/>**Default:** '' |
| `max_round=2` |  |
| `use_subcritic` | Whether to use the subcritic agent to generate subcriteria.<br/><br/>**Type:** `bool`<br/><br/>**Default:** False |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>
#### quantify_criteria
<a href="#autogen.agentchat.contrib.agent_eval.agent_eval..quantify_criteria" class="headerlink" title="Permanent link"></a>

```python
quantify_criteria(
    llm_config: dict | Literal[False] | None = None,
    criteria: list[autogen.agentchat.contrib.agent_eval.criterion.Criterion] = None,
    task: autogen.agentchat.contrib.agent_eval.task.Task = None,
    test_case: str = '',
    ground_truth: str = ''
) -> 
```

    Quantifies the performance of a system using the provided criteria.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `llm_config` | llm inference configuration.<br/><br/>**Type:** `dict \| Literal[False] \| None`<br/><br/>**Default:** None |
| `criteria` | A list of criteria for evaluating the utility of a given task.<br/><br/>**Type:** `list[autogen.agentchat.contrib.agent_eval.criterion.Criterion]`<br/><br/>**Default:** None |
| `task` | The task to evaluate.<br/><br/>**Type:** `autogen.agentchat.contrib.agent_eval.task.Task`<br/><br/>**Default:** None |
| `test_case` | The test case to evaluate.<br/><br/>**Type:** `str`<br/><br/>**Default:** '' |
| `ground_truth` | The ground truth for the test case.<br/><br/>**Type:** `str`<br/><br/>**Default:** '' |

<br />