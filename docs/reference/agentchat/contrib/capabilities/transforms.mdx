---
sidebarTitle: transforms
title: autogen.agentchat.contrib.capabilities.transforms
---

    <h2 id="autogen.agentchat.contrib.capabilities.transforms.MessageHistoryLimiter" class="doc doc-heading">
        <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
        <span class="doc doc-object-name doc-class-name">MessageHistoryLimiter</span>
        <a href="#autogen.agentchat.contrib.capabilities.transforms.MessageHistoryLimiter" class="headerlink" title="Permanent link"></a>
    </h2>

```python
MessageHistoryLimiter(max_messages: int | None = None, keep_first_message: bool = False)
```

    Limits the number of messages considered by an agent for response generation.
    
    This transform keeps only the most recent messages up to the specified maximum number of messages (max_messages).
    It trims the conversation history by removing older messages, retaining only the most recent messages.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `max_messages` | **Type:** `int \| None`<br/><br/>**Default:** None |
| `keep_first_message` | **Type:** `bool`<br/><br/>**Default:** False |

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### apply_transform
<a href="#autogen.agentchat.contrib.capabilities.transforms.MessageHistoryLimiter.apply_transform" class="headerlink" title="Permanent link"></a>

```python
apply_transform(self, messages: list[dict]) -> list[dict]
```

    Truncates the conversation history to the specified maximum number of messages.
    
    This method returns a new list containing the most recent messages up to the specified
    maximum number of messages (max_messages). If max_messages is None, it returns the
    original list of messages unmodified.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `messages` | The list of messages representing the conversation history.<br/><br/>**Type:** `list[dict]` |

<b>Returns:</b>
| Type | Description |
|--|--|
| `list[dict]` | List[Dict]: A new list containing the most recent messages up to the specified maximum. |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### get_logs
<a href="#autogen.agentchat.contrib.capabilities.transforms.MessageHistoryLimiter.get_logs" class="headerlink" title="Permanent link"></a>

```python
get_logs(
    self,
    pre_transform_messages: list[dict],
    post_transform_messages: list[dict]
) -> tuple[str, bool]
```

    

<b>Parameters:</b>
| Name | Description |
|--|--|
| `pre_transform_messages` | **Type:** `list[dict]` |
| `post_transform_messages` | **Type:** `list[dict]` |

<br />

    <h2 id="autogen.agentchat.contrib.capabilities.transforms.MessageTokenLimiter" class="doc doc-heading">
        <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
        <span class="doc doc-object-name doc-class-name">MessageTokenLimiter</span>
        <a href="#autogen.agentchat.contrib.capabilities.transforms.MessageTokenLimiter" class="headerlink" title="Permanent link"></a>
    </h2>

```python
MessageTokenLimiter(
    max_tokens_per_message: int | None = None,
    max_tokens: int | None = None,
    min_tokens: int | None = None,
    model: str = 'gpt-3.5-turbo-0613',
    filter_dict: dict | None = None,
    exclude_filter: bool = True
)
```

    Truncates messages to meet token limits for efficient processing and response generation.
    
    This transformation applies two levels of truncation to the conversation history:
    
    1. Truncates each individual message to the maximum number of tokens specified by max_tokens_per_message.
    2. Truncates the overall conversation history to the maximum number of tokens specified by max_tokens.
    
    NOTE: Tokens are counted using the encoder for the specified model. Different models may yield different token
        counts for the same text.
    
    NOTE: For multimodal LLMs, the token count may be inaccurate as it does not account for the non-text input
        (e.g images).
    
    The truncation process follows these steps in order:
    
    1. The minimum tokens threshold (`min_tokens`) is checked (0 by default). If the total number of tokens in messages
        are less than this threshold, then the messages are returned as is. In other case, the following process is applied.
    2. Messages are processed in reverse order (newest to oldest).
    3. Individual messages are truncated based on max_tokens_per_message. For multimodal messages containing both text
        and other types of content, only the text content is truncated.
    4. The overall conversation history is truncated based on the max_tokens limit. Once the accumulated token count
        exceeds this limit, the current message being processed get truncated to meet the total token count and any
        remaining messages get discarded.
    5. The truncated conversation history is reconstructed by prepending the messages to a new list to preserve the
        original message order.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `max_tokens_per_message` | Maximum number of tokens to keep in each message.<br/><br/>Must be greater than or equal to 0 if not None.<br/><br/>**Type:** `int \| None`<br/><br/>**Default:** None |
| `max_tokens` | Maximum number of tokens to keep in the chat history.<br/><br/>Must be greater than or equal to 0 if not None.<br/><br/>**Type:** `int \| None`<br/><br/>**Default:** None |
| `min_tokens` | Minimum number of tokens in messages to apply the transformation.<br/><br/>Must be greater than or equal to 0 if not None.<br/><br/>**Type:** `int \| None`<br/><br/>**Default:** None |
| `model` | The target OpenAI model for tokenization alignment.<br/><br/>**Type:** `str`<br/><br/>**Default:** 'gpt-3.5-turbo-0613' |
| `filter_dict` | A dictionary to filter out messages that you want/don't want to compress.<br/><br/>If None, no filters will be applied.<br/><br/>**Type:** `dict \| None`<br/><br/>**Default:** None |
| `exclude_filter` | If exclude filter is True (the default value), messages that match the filter will be excluded from token truncation.<br/><br/>If False, messages that match the filter will be truncated.<br/><br/>**Type:** `bool`<br/><br/>**Default:** True |

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### apply_transform
<a href="#autogen.agentchat.contrib.capabilities.transforms.MessageTokenLimiter.apply_transform" class="headerlink" title="Permanent link"></a>

```python
apply_transform(self, messages: list[dict]) -> list[dict]
```

    Applies token truncation to the conversation history.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `messages` | The list of messages representing the conversation history.<br/><br/>**Type:** `list[dict]` |

<b>Returns:</b>
| Type | Description |
|--|--|
| `list[dict]` | List[Dict]: A new list containing the truncated messages up to the specified token limits. |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### get_logs
<a href="#autogen.agentchat.contrib.capabilities.transforms.MessageTokenLimiter.get_logs" class="headerlink" title="Permanent link"></a>

```python
get_logs(
    self,
    pre_transform_messages: list[dict],
    post_transform_messages: list[dict]
) -> tuple[str, bool]
```

    

<b>Parameters:</b>
| Name | Description |
|--|--|
| `pre_transform_messages` | **Type:** `list[dict]` |
| `post_transform_messages` | **Type:** `list[dict]` |

<br />

    <h2 id="autogen.agentchat.contrib.capabilities.transforms.MessageTransform" class="doc doc-heading">
        <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
        <span class="doc doc-object-name doc-class-name">MessageTransform</span>
        <a href="#autogen.agentchat.contrib.capabilities.transforms.MessageTransform" class="headerlink" title="Permanent link"></a>
    </h2>

```python
MessageTransform(*args, **kwargs)
```

    Defines a contract for message transformation.
    
    Classes implementing this protocol should provide an `apply_transform` method
    that takes a list of messages and returns the transformed list.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `*args` |  |
| `**kwargs` |  |

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### apply_transform
<a href="#autogen.agentchat.contrib.capabilities.transforms.MessageTransform.apply_transform" class="headerlink" title="Permanent link"></a>

```python
apply_transform(self, messages: list[dict]) -> list[dict]
```

    Applies a transformation to a list of messages.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `messages` | A list of dictionaries representing messages.<br/><br/>**Type:** `list[dict]` |

<b>Returns:</b>
| Type | Description |
|--|--|
| `list[dict]` | A new list of dictionaries containing the transformed messages. |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### get_logs
<a href="#autogen.agentchat.contrib.capabilities.transforms.MessageTransform.get_logs" class="headerlink" title="Permanent link"></a>

```python
get_logs(
    self,
    pre_transform_messages: list[dict],
    post_transform_messages: list[dict]
) -> tuple[str, bool]
```

    Creates the string including the logs of the transformation
    
    Alongside the string, it returns a boolean indicating whether the transformation had an effect or not.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `pre_transform_messages` | A list of dictionaries representing messages before the transformation.<br/><br/>**Type:** `list[dict]` |
| `post_transform_messages` | A list of dictionaries representig messages after the transformation.<br/><br/>**Type:** `list[dict]` |

<b>Returns:</b>
| Type | Description |
|--|--|
| `tuple[str, bool]` | A tuple with a string with the logs and a flag indicating whether the transformation had an effect or not. |

<br />

    <h2 id="autogen.agentchat.contrib.capabilities.transforms.TextMessageCompressor" class="doc doc-heading">
        <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
        <span class="doc doc-object-name doc-class-name">TextMessageCompressor</span>
        <a href="#autogen.agentchat.contrib.capabilities.transforms.TextMessageCompressor" class="headerlink" title="Permanent link"></a>
    </h2>

```python
TextMessageCompressor(
    text_compressor: autogen.agentchat.contrib.capabilities.text_compressors.TextCompressor | None = None,
    min_tokens: int | None = None,
    compression_params: dict = \{},
    cache: autogen.cache.abstract_cache_base.AbstractCache | None = None,
    filter_dict: dict | None = None,
    exclude_filter: bool = True
)
```

    A transform for compressing text messages in a conversation history.
    
    It uses a specified text compression method to reduce the token count of messages, which can lead to more efficient
    processing and response generation by downstream models.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `text_compressor` | An instance of a class that implements the TextCompressor protocol.<br/><br/>If None, it defaults to LLMLingua.<br/><br/>**Type:** `autogen.agentchat.contrib.capabilities.text_compressors.TextCompressor \| None`<br/><br/>**Default:** None |
| `min_tokens` | Minimum number of tokens in messages to apply the transformation.<br/><br/>Must be greater than or equal to 0 if not None.<br/><br/>If None, no threshold-based compression is applied.<br/><br/>**Type:** `int \| None`<br/><br/>**Default:** None |
| `compression_params` | **Type:** `dict`<br/><br/>**Default:** \{} |
| `cache` | The cache client to use to store and retrieve previously compressed messages.<br/><br/>If None, no caching will be used.<br/><br/>**Type:** `autogen.cache.abstract_cache_base.AbstractCache \| None`<br/><br/>**Default:** None |
| `filter_dict` | A dictionary to filter out messages that you want/don't want to compress.<br/><br/>If None, no filters will be applied.<br/><br/>**Type:** `dict \| None`<br/><br/>**Default:** None |
| `exclude_filter` | If exclude filter is True (the default value), messages that match the filter will be excluded from compression.<br/><br/>If False, messages that match the filter will be compressed.<br/><br/>**Type:** `bool`<br/><br/>**Default:** True |

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### apply_transform
<a href="#autogen.agentchat.contrib.capabilities.transforms.TextMessageCompressor.apply_transform" class="headerlink" title="Permanent link"></a>

```python
apply_transform(self, messages: list[dict]) -> list[dict]
```

    Applies compression to messages in a conversation history based on the specified configuration.
    
    The function processes each message according to the `compression_args` and `min_tokens` settings, applying
    the specified compression configuration and returning a new list of messages with reduced token counts
    where possible.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `messages` | A list of message dictionaries to be compressed.<br/><br/>**Type:** `list[dict]` |

<b>Returns:</b>
| Type | Description |
|--|--|
| `list[dict]` | List[Dict]: A list of dictionaries with the message content compressed according to the configured method and scope. |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### get_logs
<a href="#autogen.agentchat.contrib.capabilities.transforms.TextMessageCompressor.get_logs" class="headerlink" title="Permanent link"></a>

```python
get_logs(
    self,
    pre_transform_messages: list[dict],
    post_transform_messages: list[dict]
) -> tuple[str, bool]
```

    

<b>Parameters:</b>
| Name | Description |
|--|--|
| `pre_transform_messages` | **Type:** `list[dict]` |
| `post_transform_messages` | **Type:** `list[dict]` |

<br />

    <h2 id="autogen.agentchat.contrib.capabilities.transforms.TextMessageContentName" class="doc doc-heading">
        <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
        <span class="doc doc-object-name doc-class-name">TextMessageContentName</span>
        <a href="#autogen.agentchat.contrib.capabilities.transforms.TextMessageContentName" class="headerlink" title="Permanent link"></a>
    </h2>

```python
TextMessageContentName(
    position: str = 'start',
    format_string: str = '\{name}:\n',
    deduplicate: bool = True,
    filter_dict: dict | None = None,
    exclude_filter: bool = True
)
```

    A transform for including the agent's name in the content of a message.
    
        How to create and apply the transform:
        # Imports
        from autogen.agentchat.contrib.capabilities import transform_messages, transforms
    
        # Create Transform
        name_transform = transforms.TextMessageContentName(position="start", format_string="'\{name}' said:
    ")
    
        # Create the TransformMessages
        context_handling = transform_messages.TransformMessages(
                    transforms=[
                        name_transform
                    ]
                )
    
        # Add it to an agent so when they run inference it will apply to the messages
        context_handling.add_to_agent(my_agent)

<b>Parameters:</b>
| Name | Description |
|--|--|
| `position` | The position to add the name to the content.<br/><br/>The possible options are 'start' or 'end'.<br/><br/>Defaults to 'start'.<br/><br/>**Type:** `str`<br/><br/>**Default:** 'start' |
| `format_string` | The f-string to format the message name with.<br/><br/>Use '\\{name}' as a placeholder for the agent's name.<br/><br/>Defaults to '\\{name}:<br/><br/>**Type:** `str`<br/><br/>**Default:** '\{name} |
| `deduplicate` | Whether to deduplicate the formatted string so it doesn't appear twice (sometimes the LLM will add it to new messages itself).<br/><br/>Defaults to True.<br/><br/>**Type:** `bool`<br/><br/>**Default:** True |
| `filter_dict` | A dictionary to filter out messages that you want/don't want to compress.<br/><br/>If None, no filters will be applied.<br/><br/>**Type:** `dict \| None`<br/><br/>**Default:** None |
| `exclude_filter` | If exclude filter is True (the default value), messages that match the filter will be excluded from compression.<br/><br/>If False, messages that match the filter will be compressed.<br/><br/>**Type:** `bool`<br/><br/>**Default:** True |

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### apply_transform
<a href="#autogen.agentchat.contrib.capabilities.transforms.TextMessageContentName.apply_transform" class="headerlink" title="Permanent link"></a>

```python
apply_transform(self, messages: list[dict]) -> list[dict]
```

    Applies the name change to the message based on the position and format string.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `messages` | A list of message dictionaries.<br/><br/>**Type:** `list[dict]` |

<b>Returns:</b>
| Type | Description |
|--|--|
| `list[dict]` | List[Dict]: A list of dictionaries with the message content updated with names. |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### get_logs
<a href="#autogen.agentchat.contrib.capabilities.transforms.TextMessageContentName.get_logs" class="headerlink" title="Permanent link"></a>

```python
get_logs(
    self,
    pre_transform_messages: list[dict],
    post_transform_messages: list[dict]
) -> tuple[str, bool]
```

    

<b>Parameters:</b>
| Name | Description |
|--|--|
| `pre_transform_messages` | **Type:** `list[dict]` |
| `post_transform_messages` | **Type:** `list[dict]` |

<br />